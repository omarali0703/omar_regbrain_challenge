{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Omar Ali's Submission\n",
        "\n",
        "## Plan for Cube\n",
        "\n",
        "> For the consideration of time, I have pre-generated RST trees from a selection of instances within the provided dataset. In practice this would be scaled up to an automated generation cycle per instance whereby, for each instance added to the database, and RST tree would be generated.\n",
        "> \n",
        "> For each instance, the RST tree would be weighted with the most important parts considered highly above the less important sections. Following a similar model to text summarisation, the least important aspects will be removed.\n",
        "> \n",
        "> With the most important parts identified we can train a model to determine what features are present within topics that are indicative of emerging changes required by policymakers. This model allows for our analysis to be scaled to different instances that are coming through every day. \n",
        "> \n",
        "> The hypothesis is that there is some level of consistency within information indicative of a policy change. I hope that the added level of rhetorical structure theory and its segmentation will allow for these parts to be highlighted and learned from.\n",
        "> \n",
        "> In future, I may also consider using the relationships between spans of text as an indication to emerging policies. For example relationships that represent attributions or cause-and-effect might likely be an indication of emerging changes to policy.\n",
        "\n",
        "## Implementation\n",
        "1. Weighting logic to rank our RST trees\n",
        "2. Data taken from the data set processes using HILDA (and RST parser)\n",
        "\n",
        "Output:\n",
        "1. Ranked, indexed list of the most important, salient points taken from the text that are likely to indicate policy changes."
      ],
      "metadata": {
        "id": "k3KZ4rFOwvnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing dependencies needed"
      ],
      "metadata": {
        "id": "dVdOXx3dx-3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "E7U-uUkjx8N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Â RST Ranker Logic\n",
        "This Section of the code aims to rank EDUs based on their importance with the text. Text must first be processed using HILDA and stored."
      ],
      "metadata": {
        "id": "rfxmWGh7xn9N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isn0BXdooUgk"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import re \n",
        "import json\n",
        "def convert_str_RST_to_tup(rst_str = None):\n",
        "    rst_str = rst_str.replace('ParseTree', '')\n",
        "    print (rst_str)\n",
        "    rst_str = eval(rst_str)\n",
        "    return rst_str\n",
        "\n",
        "w = []\n",
        "# The two weighting schemes defined. HEAVY removes all SATELITES\n",
        "# (think text summerization), LIGHT only intensifies and subdues NUC's and SAT's.\n",
        "\n",
        "HEAVY = (1, 0)\n",
        "LIGHT = (1.4, 0.6)\n",
        "STAGE_2 = (1, 1)\n",
        "\n",
        "# Recursively weight the the tuple-tree (nested tuples). \n",
        "def weight_RST(weights, tup_obj, weight=1, weighting_scheme=LIGHT):\n",
        "    # print (\"Begin Weighting Process....\")\n",
        "    NUC = '[N]'\n",
        "    SAT = '[S]'\n",
        "    NA  = 'n/a'\n",
        "    relation = tup_obj[0]\n",
        "    subtree = tup_obj[1]\n",
        "    relation_type = re.findall(r'\\[[a-zA-Z]\\]', relation)\n",
        "\n",
        "    if relation == NA:\n",
        "        weights.append([subtree[0], weighting_scheme[0]])\n",
        "        return\n",
        "\n",
        "    if isinstance(subtree[0], str):\n",
        "        if relation_type[0] == NUC:\n",
        "            weights.append ([subtree[0], weight*weighting_scheme[0]])\n",
        "        if relation_type[0] == SAT:\n",
        "            weights.append ([subtree[0], weight*weighting_scheme[1]])\n",
        "    if isinstance(subtree[1], str):\n",
        "        if relation_type[1] == NUC:\n",
        "            weights.append ([subtree[1], weight*weighting_scheme[0]])\n",
        "        if relation_type[1] == SAT:\n",
        "            weights.append ([subtree[1], weight*weighting_scheme[1]])\n",
        "    if isinstance(subtree[1], tuple):\n",
        "        if relation_type[1] == NUC:\n",
        "            weight_RST(weights, subtree[1], weight*weighting_scheme[0])\n",
        "        if relation_type[1] == SAT:\n",
        "            weight_RST(weights, subtree[1], weight*weighting_scheme[1])\n",
        "    if isinstance(subtree[0], tuple):\n",
        "        if relation_type[0] == NUC:\n",
        "            weight_RST(weights, subtree[0], weight*weighting_scheme[0])\n",
        "        if relation_type[0] == SAT:\n",
        "            weight_RST(weights, subtree[0], weight*weighting_scheme[1])\n",
        "    # print (\"FINISHED Weighting Process....\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Representation of outputs\n",
        "\n",
        "> Below is a graph that describes which parts of the text are the most important for each instance.\n",
        "> \n",
        "> We threshold this information and use it to train our subsequent model for detection of emerging policy changing data."
      ],
      "metadata": {
        "id": "_aPAtHF4yIJX"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "submission.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}